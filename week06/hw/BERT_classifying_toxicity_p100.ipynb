{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will continue on the [Conversation AI](https://conversationai.github.io/) dataset seen in [week 4 homework and lab](https://github.com/MIDS-scaling-up/v2/tree/master/week04). \n",
    "We shall use a version of pytorch BERT for classifying comments found at [https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT).  \n",
    "\n",
    "The original implementation of BERT is optimised for TPU. Google released some amazing performance improvements on TPU over GPU, for example, see [here](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78) - *BERT relies on massive compute for pre-training ( 4 days on 4 to 16 Cloud TPUs; pre-training on 8 GPUs would take 40–70 days).*. In response, Nvidia released [apex](https://devblogs.nvidia.com/apex-pytorch-easy-mixed-precision-training/), which gave mixed precision training. Weights are stored in float32 format, but calculations, like forward and backward propagation happen in float16 - this allows these calculations to be made with a [4X speed up](https://github.com/huggingface/pytorch-pretrained-BERT/issues/149).  \n",
    "\n",
    "We shall apply BERT to the problem for classifiying toxicity, using apex from Nvidia. We shall compare the impact of hardware by running the model on a V100 and P100 and comparing the speed and accuracy in both cases.   \n",
    "\n",
    "This script relies heavily on an existing [Kaggle kernel](https://www.kaggle.com/yuval6967/toxic-bert-plain-vanila) from [yuval r](https://www.kaggle.com/yuval6967). \n",
    "  \n",
    "*Disclaimer: the dataset used contains text that may be considered profane, vulgar, or offensive.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "from apex import amp\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tables in /opt/conda/lib/python3.6/site-packages (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in /opt/conda/lib/python3.6/site-packages (from tables) (1.16.3)\n",
      "Requirement already satisfied, skipping upgrade: numexpr>=2.6.2 in /opt/conda/lib/python3.6/site-packages (from tables) (2.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's activate CUDA for GPU based operations\n",
    "device=torch.device('cuda')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the PATH variable to whereever your `week06/hw` directory is located.  \n",
    "**For the final run we would like you to have a train_size of at least 1 Million rows, and a valid size of at least 500K rows. When you first run the script, feel free to work with a reduced train and valid size for speed.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In bert we need all inputs to have the same length, we will use the first 220 characters. \n",
    "class bert_training():\n",
    "    def __init__(self,train_size,val_size,\n",
    "                max_seq_length=220,seed=1234,parent_dir_path='/root/v2/week06/hw',\n",
    "                bert_tf_model='uncased_L-12_H-768_A-12'):\n",
    "        self._MAX_SEQUENCE_LENGTH = max_seq_length\n",
    "        self._SEED = seed\n",
    "        self._PATH = parent_dir_path\n",
    "        self._DATA_DIR = os.path.join(self._PATH, \"data\")\n",
    "        self._WORK_DIR = os.path.join(self._PATH, \"workingdir\")\n",
    "        self._train_size=train_size\n",
    "        self._val_size=val_size\n",
    "        self._BERT_MODEL_PATH = os.path.join(self._DATA_DIR, bert_tf_model)\n",
    "        self._tokenizer = BertTokenizer.from_pretrained(self._BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "\n",
    "    def tf_to_pytorch_model(self,pytorch_model_bin='pytorch_model.bin'):\n",
    "        BERT_MODEL_PATH = os.path.join(self._DATA_DIR, self._BERT_MODEL_PATH)\n",
    "        convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "                                    os.path.join(self._BERT_MODEL_PATH, 'bert_model.ckpt'),\n",
    "                                    os.path.join(self._BERT_MODEL_PATH, 'bert_config.json'), \n",
    "                                    os.path.join(self._WORK_DIR, pytorch_model_bin))\n",
    "\n",
    "        shutil.copyfile(os.path.join(BERT_MODEL_PATH, 'bert_config.json'), \\\n",
    "                        os.path.join(self._WORK_DIR, 'bert_config.json'))\n",
    "        # This is the Bert configuration file\n",
    "        bert_config = BertConfig(os.path.join(self._WORK_DIR, 'bert_config.json'))\n",
    "        return bert_config\n",
    "    \n",
    "    def convert_lines(self,example):\n",
    "        tokenizer = self._tokenizer\n",
    "        max_seq_length = self._MAX_SEQUENCE_LENGTH-2\n",
    "        all_tokens = []\n",
    "        longer = 0\n",
    "        for text in tqdm_notebook(example):\n",
    "            tokens_a = tokenizer.tokenize(text)\n",
    "            if len(tokens_a)>max_seq_length:\n",
    "                tokens_a = tokens_a[:max_seq_length]\n",
    "                longer += 1\n",
    "            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "            all_tokens.append(one_token)\n",
    "        print(longer)\n",
    "        return np.array(all_tokens)\n",
    "    \n",
    "    def predict_from_pretrained_model(self):\n",
    "        bert = BertModel.from_pretrained(self._WORK_DIR).cuda()\n",
    "        bert_output = bert(torch.tensor([input_ids]).cuda())\n",
    "        return bert_output\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        tokens = self._tokenizer.tokenize(text)\n",
    "        tokens_bert = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens_bert)\n",
    "        return input_ids,tokens,tokens_bert\n",
    "    \n",
    "\n",
    "    def initialize_model_for_training(self,num_labels,EPOCHS=1,model_seed=21000,lr=2e-5,batch_size=32,\n",
    "                                      accumulation_steps=2):\n",
    "        # Setup model parameters\n",
    "        np.random.seed(model_seed)\n",
    "        torch.manual_seed(model_seed)\n",
    "        torch.cuda.manual_seed(model_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(self._WORK_DIR,cache_dir=None,num_labels=num_labels)\n",
    "        model.zero_grad()\n",
    "        model = model.to(device)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        train = train_dataset\n",
    "        num_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n",
    "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                             lr=lr,\n",
    "                             warmup=0.05,\n",
    "                             t_total=num_train_optimization_steps)\n",
    "\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "        model=model.train()\n",
    "        return model,optimizer,EPOCHS\n",
    "\n",
    "    def run_training(self,model,train,optimizer,EPOCHS=1,batch_size=32,accumulation_steps=2):\n",
    "        tq = tqdm_notebook(range(EPOCHS))\n",
    "        for epoch in tq:\n",
    "            train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "            avg_loss = 0.\n",
    "            avg_accuracy = 0.\n",
    "            lossf=None\n",
    "            tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "            optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n",
    "            for i,(x_batch, y_batch) in tk0:\n",
    "                y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
    "                loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                    optimizer.step()                            # Now we can do an optimizer step\n",
    "                    optimizer.zero_grad()\n",
    "                if lossf:\n",
    "                    lossf = 0.98*lossf+0.02*loss.item()\n",
    "                else:\n",
    "                    lossf = loss.item()\n",
    "                tk0.set_postfix(loss = lossf)\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "                avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
    "            tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "            return model\n",
    "       \n",
    "    def predict(self,model,X_val,batch_size=32):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=False\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((len(X_val)))\n",
    "        valid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        tk0 = tqdm_notebook(valid_loader)\n",
    "        for i,(x_batch,)  in enumerate(tk0):\n",
    "            pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
    "            valid_preds[i*batch_size:(i+1)*batch_size]=pred[:,0].detach().cpu().squeeze().numpy()\n",
    "        return valid_preds\n",
    "    \n",
    "    def compute_auc_score(self,y, predictions):\n",
    "        return roc_auc_score(y, predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be the files you downloaded earlier when you ran `download.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from pytorch_pretrained_bert import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['download.sh',\n",
       " 'cased_L-12_H-768_A-12',\n",
       " 'test.csv',\n",
       " 'train.csv',\n",
       " 'uncased_L-12_H-768_A-12']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_size = 10000\n",
    "# valid_size = 5000\n",
    "\n",
    "train_size = 1000000\n",
    "valid_size = 500000\n",
    "\n",
    "bert_obj = bert_training(train_size=train_size,val_size=valid_size) # create an instance of the setup class\n",
    "DATA_DIR = bert_obj._DATA_DIR\n",
    "WORK_DIR = bert_obj._WORK_DIR\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall install pytorch BERT implementation.   \n",
    "If you would like to experiment with or view any code (purely optional, and not graded :) ), you can copy the files from the repo https://github.com/huggingface/pytorch-pretrained-BERT  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now load the model. When you run this, comment out the `capture` command to understand the archecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "bert_config1 = bert_obj.tf_to_pytorch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bert_config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Translate model from tensorflow to pytorch\n",
    "# BERT_MODEL_PATH = os.path.join(model1._DATA_DIR, 'uncased_L-12_H-768_A-12')\n",
    "# convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "#                             os.path.join(BERT_MODEL_PATH, 'bert_model.ckpt'),\n",
    "#                             os.path.join(BERT_MODEL_PATH, 'bert_config.json'), \n",
    "#                             os.path.join(model1._WORK_DIR, 'pytorch_model.bin'))\n",
    "\n",
    "# shutil.copyfile(os.path.join(BERT_MODEL_PATH, 'bert_config.json'), \\\n",
    "#                 os.path.join(model1._WORK_DIR, 'bert_config.json'))\n",
    "# # This is the Bert configuration file\n",
    "# bert_config2 = BertConfig(os.path.join(model1._WORK_DIR, 'bert_config.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert needs a special formatting of sentences, so we have a sentence start and end token, as well as separators.   \n",
    "Thanks to this [script](https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming) for a fast convertor of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>This method is defined in the class \"bert_training\" above.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_lines(example, max_seq_length,tokenizer):\n",
    "#     max_seq_length -=2\n",
    "#     all_tokens = []\n",
    "#     longer = 0\n",
    "#     for text in tqdm_notebook(example):\n",
    "#         tokens_a = tokenizer.tokenize(text)\n",
    "#         if len(tokens_a)>max_seq_length:\n",
    "#             tokens_a = tokens_a[:max_seq_length]\n",
    "#             longer += 1\n",
    "#         one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "#         all_tokens.append(one_token)\n",
    "#     print(longer)\n",
    "#     return np.array(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the BERT tokenizer and convert the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Updated the following code cell to make tokenization more efficient. The reference implementation did not accout for runs spread across multiple days when the notebook had to be shut down or kernel has to restart due to OOM issues. Storing the tokenized DF on disk in HDFS allows for very fast load time for subsequent runs without running the tokenizer again. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I-- HDFS store /root/v2/week06/hw/workingdir/train_all-1000000.h5 doesn't exist; Regenerating tokenized dataframe!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b934d86ccc4feb80378cd7b8fe7a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1500000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "33724\n",
      "loaded 1500000 records\n",
      "CPU times: user 34min 1s, sys: 14.7 s, total: 34min 16s\n",
      "Wall time: 34min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SEED = bert_obj._SEED\n",
    "# Only do the following when you don't have the data frame saved to disk \n",
    "store_file = WORK_DIR+'/train_all-'+str(train_size)+'.h5'\n",
    "np_store = WORK_DIR+'/sequences-'+str(train_size)\n",
    "\n",
    "if not os.path.exists(store_file): # if stores don't exist already\n",
    "    print(\"--I-- HDFS store \"+store_file+\" doesn't exist; Regenerating tokenized dataframe!\")\n",
    "    # Tokenize training data and load into pandas \n",
    "    train_all = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\")).sample(train_size+valid_size,random_state=SEED)\n",
    "    sequences = bert_obj.convert_lines(train_all[\"comment_text\"].fillna(\"DUMMY_VALUE\"))\n",
    "    train_all=train_all.fillna(0)\n",
    "    # Make sure all comment_text values are strings\n",
    "    train_all['comment_text'] = train_all['comment_text'].astype(str) \n",
    "    \n",
    "    # Store the pandas dataframe to HDFS file system & numpy to binary file\n",
    "    store = pd.HDFStore(store_file)\n",
    "    store['train_all'] = train_all\n",
    "    np.save(np_store,sequences,allow_pickle=True)\n",
    "    \n",
    "else:\n",
    "    ## Load into memory from HDFS file\n",
    "    print(\"--I-- HDFS store \"+store_file+\"exists; Loading to pandas dataframe in memory\")\n",
    "    store = pd.HDFStore(store_file)\n",
    "    train_all = store['train_all']\n",
    "    sequences=np.load(np_store+\".npy\",allow_pickle=True)\n",
    "    \n",
    "print('loaded %d records' % len(train_all))\n",
    "\n",
    "# Check the loaded data is as expected for downstream tasks\n",
    "assert len(sequences) == (train_size+valid_size), \"Sequences loaded size is not correct!\"\n",
    "assert len(train_all) == (train_size+valid_size), \"train_all loaded size is not correct!\"\n",
    "assert type(train_all['comment_text'].iloc[0]) == str , \"Comment text in train_all is not string type!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how the tokenising works in BERT, see below how it recongizes misspellings - words the model never saw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458232</th>\n",
       "      <td>It's difficult for many old people to keep up ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272766</th>\n",
       "      <td>She recognized that her tiny-handed husband is...</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339129</th>\n",
       "      <td>HPHY76,\\nGood for you for thinking out loud, w...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773565</th>\n",
       "      <td>And I bet that in the day you expected your Je...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476233</th>\n",
       "      <td>Kennedy will add a much needed and scientifica...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text    target\n",
       "458232  It's difficult for many old people to keep up ...  0.000000\n",
       "272766  She recognized that her tiny-handed husband is...  0.166667\n",
       "339129  HPHY76,\\nGood for you for thinking out loud, w...  0.000000\n",
       "773565  And I bet that in the day you expected your Je...  0.500000\n",
       "476233  Kennedy will add a much needed and scientifica...  0.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all[[\"comment_text\", 'target']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tokenize some text (I intentionally mispelled some words to check berts subword information handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi , i am learning new things in w ##25 ##1 about deep learning the cloud and te ##h edge .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hi, I am learning new things in w251 about deep learning the cloud and teh edge.'\n",
    "input_ids,tokens,tokens_bert = bert_obj.tokenize(text)\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added start and end token and convert to ids. This is how it is fed into BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi , i am learning new things in w ##25 ##1 about deep learning the cloud and te ##h edge . [SEP]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'101 7632 1010 1045 2572 4083 2047 2477 1999 1059 17788 2487 2055 2784 4083 1996 6112 1998 8915 2232 3341 1012 102'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens1 = [\"[CLS]\"] + tokens1 + [\"[SEP]\"]\n",
    "' '.join(tokens_bert)\n",
    "' '.join(map(str, input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When BERT converts this sentence to a torch tensor below is shape of the stored tensors.  \n",
    "We have 12 input tensors, while the sentence tokens has length 23; where are can you see the 23 tokens in the tensors ?... **Feel free to post in slack or discuss in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokens ['hi', ',', 'i', 'am', 'learning', 'new', 'things', 'in', 'w', '##25', '##1', 'about', 'deep', 'learning', 'the', 'cloud', 'and', 'te', '##h', 'edge', '.']\n",
      "Number of tokens 21\n",
      "Tensor shapes : [(1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768)]\n",
      "Number of torch tensors : 12\n"
     ]
    }
   ],
   "source": [
    "# put input on gpu and make prediction\n",
    "bert_output = bert_obj.predict_from_pretrained_model()\n",
    "print('Sentence tokens {}'.format(tokens))\n",
    "print('Number of tokens {}'.format(len(tokens)))\n",
    "print('Tensor shapes : {}'.format([b.cpu().detach().numpy().shape for b in bert_output[0]]))\n",
    "print('Number of torch tensors : {}'.format(len(bert_output[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a binary problem, we change our target to [0,1], instead of float.   \n",
    "We also split the dataset into a training and validation set, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all['target']=(train_all['target']>=0.5).astype(float)\n",
    "# Training data - sentences\n",
    "X = sequences[:train_size] \n",
    "# Target - the toxicity. \n",
    "y = train_all[['target']].values[:train_size]\n",
    "X_val = sequences[train_size:]                \n",
    "y_val = train_all[['target']].values[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=train_all.tail(valid_size).copy()\n",
    "train_df=train_all.head(train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From here on in we would like you to run BERT.**   \n",
    "**Please do rely on the script available -  [Kaggle kernel](https://www.kaggle.com/yuval6967/toxic-bert-plain-vanila) from [yuval r](https://www.kaggle.com/yuval6967) - for at least the first few steps up to training and prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1)**   \n",
    "**Load the training set to a training dataset. For this you need to load the X sequences and y objects to torch tensors**   \n",
    "**You can use `torch.utils.data.TensorDataset` to input these into a train_dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data creations\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)**  \n",
    "**Set your learning rate and batch size; and optionally random seeds if you want reproducable results**   \n",
    "**Load your pretrained BERT using `BertForSequenceClassification`**   \n",
    "**Initialise the gradients and place the model on cuda, set up your optimiser and decay parameters**\n",
    "**Initialise the model with `apex` (we imprted this as `amp`) for mixed precision training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for training \n",
    "model1,optimizer1,epochs = bert_obj.initialize_model_for_training(y.shape[1],EPOCHS=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)**  \n",
    "**Start training your model by iterating through batches in a single epoch of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--I Training model on training size:  1000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ce1a3f4add4244ad26086815e4e05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 51min 16s, sys: 2h 14min 11s, total: 6h 5min 27s\n",
      "Wall time: 6h 5min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "model_file_name = WORK_DIR+\"/bert_pytorch_p100a_train\"+str(train_size)+\".bin\"\n",
    "if not os.path.exists(model_file_name):\n",
    "    print(\"--I Training model on training size: \", train_size)\n",
    "    model1=bert_obj.run_training(model1,train_dataset,optimizer1,EPOCHS=epochs)\n",
    "else:\n",
    "    print(\"--I Training model on training size:%d exists on disk!\" % train_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)**  \n",
    "**Store your trained model to disk, you will need it if you choose section 8C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_file_name):\n",
    "    output_model_file = WORK_DIR+\"/bert_pytorch_p100a_train\"+str(train_size)+\".bin\"\n",
    "    torch.save(model1.state_dict(), output_model_file)\n",
    "else:\n",
    "    print(\"--I Training model on training size:%d exists on disk!\" % train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)**   \n",
    "**Now make a prediction for your validation set.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# The following 2 lines are not needed but show how to download the model for prediction\n",
    "if os.path.exists(model_file_name):\n",
    "    model = BertForSequenceClassification(bert_config1,num_labels=y.shape[1])\n",
    "    model.load_state_dict(torch.load(model_file_name))\n",
    "    model.to(device)\n",
    "else:\n",
    "    print(\"--E-- Model doesn't exist! Either retrain the model or copy binary to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7700b1e702a5414d95ce8942eb07842d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15625), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38min 16s, sys: 22min 28s, total: 1h 45s\n",
      "Wall time: 1h 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions=bert_obj.predict(model,X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)**  \n",
    "**In the yuval's kernel he get a metric based on the metric for the jigsaw competition - it is quite complicated. Instead, we would like you to measure the `AUC`, similar to how you did in homework 04. You can compare the results to HW04**  \n",
    "*A tip, if your score is lower than homework 04 something is wrong....*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score =  0.97003\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.sigmoid(torch.tensor(predictions)).numpy()\n",
    "auc_score = bert_obj.compute_auc_score(y_val,predictions)\n",
    "print(\"AUC score = \" , round(auc_score,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)**  \n",
    "**Can you show/print the validation sentences predicted with the highest and lowest toxicity ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  1\n",
      "True Target Value: 0.0 Predicted Target Value: 0.9997\n",
      "\n",
      "In particular, Republicans seem to think that it is fiscally prudent to cut taxes, and to skew these tax cuts to the wealthy. They belong to the Church of Arthur Laffer (Reformed). The Gospel according to Laffer says that reducing taxes, up to a point, will increase government spending. There is some evidence that Lafferism is true, but the followers of the Reformed Church of Laffer ignore the bit about \"up to a point\". They use the Lafferite Gospel to justify any and all tax cuts. This is fiscally irresponsible, but few Republicans seem to have any real knowledge of economics.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  2\n",
      "True Target Value: 1.0 Predicted Target Value: 0.9997\n",
      "\n",
      "\"White House unveils ‘Made in America’ week, though many Trump products are made overseas\"\n",
      "\n",
      "That's soooo unfair to point out Trumpian hypocrisy.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  3\n",
      "True Target Value: 1.0 Predicted Target Value: 0.9997\n",
      "\n",
      "Louis you look like a dam n fool with that man pony tail and baggy aloha shirt. C'man!!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  4\n",
      "True Target Value: 0.0 Predicted Target Value: 0.9997\n",
      "\n",
      "Still unable to deal with illegal fishing licensing of persons unlawfully in the State.  Thanks to Senator Laura Thieland, Karl Rhoads and Representative Kaniela Ing.  Each gave it a good try. Not this year... but possibly at another time. Each of you will be remembered for you willingness to fight injustice against what has been termed by the AP as \"floating prisons\" and in a second article titled \"is the State breaking it's own law.. Meanwhile, the DLNR, the law breaker, should not accept passports from a third person (captain or owner) of a vessel that has crew members that are detained on board that have been denied entry into the USA.  This will eventually catch up with the State and bring embarrassment and expose what may even be illegality. Dr. Bruce Anderson at DAR... you need to follow the law as it is currently written.  Specifically HRS 189-5. The legislature has put DLNR on notice and should follow up with a letter indicating it's findings.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  5\n",
      "True Target Value: 0.0 Predicted Target Value: 0.9997\n",
      "\n",
      "When's the Run for Men? Men account for 97% of combat deaths, 93% of industrial accident deaths, 76% of homicide victims, and 80% of suicide deaths...male privilege I guess.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idx_most_toxic=predictions.argsort()[-5:][::-1]\n",
    "for index,row in enumerate(idx_most_toxic):\n",
    "    print(\"Sentence: \",index+1)\n",
    "    print(\"True Target Value:\",train_all.iloc[row].target,\"Predicted Target Value:\",round(predictions[row],4))\n",
    "    print()\n",
    "    print(train_all.iloc[row].comment_text)\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  1\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "AceandGary,  I wouldn't be surprised either.  I won't be surprised if he calls in sick and cancelled..\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  2\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "\"It is the President's option to select the judges for the Supreme Court is it not?\".... Hmmmm, I recall the last president made a selection and the Republican Party refused to even give the selection a hearing. I also recall that selection was in the past highly supported by both parties. The real problem is the extremism in congress (Mitch McConnell; Paul Ryan; Nancy Pelosie) who put party and re-election above all.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  3\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "No he is correct. Any military school teaches terrorism, it is used by a weaker combatant to provoke a disproportionate response from the stronger enemy and create anger in the local population. If they can generate an large enough response the stronger combatant can lose since the local population will turn against them.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  4\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "It is wicked difficult to land a teaching position at a regional law school, a significant investment in time and effort. Resignation should not the first choice of either Prof. Shurtz, nor others who've also invested significant investment in time and effort in pursuit of the same goal -- landing a teaching position at a regional law school. Due process is not something restricted to the criminal courts, it is also present in public education -- for students, staff, and faculty.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  5\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "No, it's the \"Church of Love\".\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idx_least_toxic=predictions.argsort()[:5]\n",
    "for index,row in enumerate(idx_least_toxic):\n",
    "    print(\"Sentence: \",index+1)\n",
    "    print(\"True Target Value:\",train_all.iloc[row].target,\"Predicted Target Value:\",round(predictions[row],4))\n",
    "    print()\n",
    "    print(train_all.iloc[row].comment_text)\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8)**  \n",
    "**Pick only one of the below items and complete it. The last two will take a good amount of time (and partial success on them is fine), so proceed with caution on your choice of items :)** \n",
    "  \n",
    "  \n",
    "**A. Can you train on two epochs ?**\n",
    "\n",
    "**B. Can you change the learning rate and improve validation score ?**\n",
    "   \n",
    "**C. Make a prediction on the test data set with your downloaded model and submit to Kaggle to see where you score on public LB - check out [Abhishek's](https://www.kaggle.com/abhishek) script - https://www.kaggle.com/abhishek/pytorch-bert-inference . Note, you will need to fork Abhisheks kernel, swap out the weights to your downloaded weights and commit the kernel. When finalised and you get the output, there is a button to submit to the competition**  \n",
    "  \n",
    "**D. Get BERT running on the tx2 for a sample of the data.** \n",
    "  \n",
    "**E. Finally, and very challenging -- the `BertAdam` optimiser proved to be suboptimal for this task. There is a better optimiser for this dataset in this script [here](https://www.kaggle.com/cristinasierra/pretext-lstm-tuning-v3). Check out the `custom_loss` function. Can you implement it ? It means getting under the hood of the `BertForSequenceClassification` at the source repo and implementing a modified version locally .  `https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Training on 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad0a2ff7e444403b941a3c98f4cf923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcfed51f5044d179994f3b73de61ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15625), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score =  0.96942\n",
      "CPU times: user 4h 34min 47s, sys: 2h 34min 36s, total: 7h 9min 23s\n",
      "Wall time: 7h 9min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize \n",
    "model2,optimizer2,epochs = bert_obj.initialize_model_for_training(y.shape[1],EPOCHS=2)\n",
    "\n",
    "# Train and save the model\n",
    "model2=bert_obj.run_training(model2,train_dataset,optimizer2,EPOCHS=epochs)\n",
    "output_model_file = WORK_DIR+\"/bert_pytorch_p100a_train\"+str(train_size)+\"_epochs_\"+str(epochs)+\".bin\"\n",
    "torch.save(model2.state_dict(), output_model_file)\n",
    "\n",
    "# Make the predictions\n",
    "predictions = bert_obj.predict(model2,X_val)\n",
    "predictions = torch.sigmoid(torch.tensor(predictions)).numpy() # add a final sigmoid layer\n",
    "auc_score = bert_obj.compute_auc_score(y_val,predictions)\n",
    "print(\"AUC score = \" , round(auc_score,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Change the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f26b821d3a8405c984b535ad224c556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7f16e07ebd45f08729c12497aecbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15625), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score =  0.96299\n",
      "CPU times: user 4h 38min 28s, sys: 2h 32min 31s, total: 7h 11min\n",
      "Wall time: 7h 10min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set learning rate \n",
    "lr = 2e-6\n",
    "\n",
    "# Initialize \n",
    "model3,optimizer3,epochs = bert_obj.initialize_model_for_training(y.shape[1],EPOCHS=1,lr=lr)\n",
    "\n",
    "# Train and save the model\n",
    "model3=bert_obj.run_training(model3,train_dataset,optimizer3,EPOCHS=epochs)\n",
    "output_model_file = WORK_DIR+\"/bert_pytorch_p100a_train\"+str(train_size)+\"_lr_\"+str(lr)+\"_epochs_\"+str(epochs)+\".bin\"\n",
    "torch.save(model3.state_dict(), output_model_file)\n",
    "\n",
    "# Make the predictions\n",
    "predictions = bert_obj.predict(model3,X_val)\n",
    "predictions = torch.sigmoid(torch.tensor(predictions)).numpy() # add a final sigmoid layer\n",
    "auc_score = bert_obj.compute_auc_score(y_val,predictions)\n",
    "print(\"AUC score = \" , round(auc_score,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
