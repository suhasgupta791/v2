{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will continue on the [Conversation AI](https://conversationai.github.io/) dataset seen in [week 4 homework and lab](https://github.com/MIDS-scaling-up/v2/tree/master/week04). \n",
    "We shall use a version of pytorch BERT for classifying comments found at [https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT).  \n",
    "\n",
    "The original implementation of BERT is optimised for TPU. Google released some amazing performance improvements on TPU over GPU, for example, see [here](https://medium.com/@ranko.mosic/googles-bert-nlp-5b2bb1236d78) - *BERT relies on massive compute for pre-training ( 4 days on 4 to 16 Cloud TPUs; pre-training on 8 GPUs would take 40â€“70 days).*. In response, Nvidia released [apex](https://devblogs.nvidia.com/apex-pytorch-easy-mixed-precision-training/), which gave mixed precision training. Weights are stored in float32 format, but calculations, like forward and backward propagation happen in float16 - this allows these calculations to be made with a [4X speed up](https://github.com/huggingface/pytorch-pretrained-BERT/issues/149).  \n",
    "\n",
    "We shall apply BERT to the problem for classifiying toxicity, using apex from Nvidia. We shall compare the impact of hardware by running the model on a V100 and P100 and comparing the speed and accuracy in both cases.   \n",
    "\n",
    "This script relies heavily on an existing [Kaggle kernel](https://www.kaggle.com/yuval6967/toxic-bert-plain-vanila) from [yuval r](https://www.kaggle.com/yuval6967). \n",
    "  \n",
    "*Disclaimer: the dataset used contains text that may be considered profane, vulgar, or offensive.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "from apex import amp\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's activate CUDA for GPU based operations\n",
    "device=torch.device('cuda')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the PATH variable to whereever your `week06/hw` directory is located.  \n",
    "**For the final run we would like you to have a train_size of at least 1 Million rows, and a valid size of at least 500K rows. When you first run the script, feel free to work with a reduced train and valid size for speed.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In bert we need all inputs to have the same length, we will use the first 220 characters. \n",
    "class bert_training():\n",
    "    def __init__(self,train_size,val_size,\n",
    "                max_seq_length=220,seed=1234,parent_dir_path='/root/v2/week06/hw',\n",
    "                bert_tf_model='uncased_L-12_H-768_A-12'):\n",
    "        self._MAX_SEQUENCE_LENGTH = max_seq_length\n",
    "        self._SEED = seed\n",
    "        self._PATH = parent_dir_path\n",
    "        self._DATA_DIR = os.path.join(self._PATH, \"data\")\n",
    "        self._WORK_DIR = os.path.join(self._PATH, \"workingdir\")\n",
    "        self._train_size=train_size\n",
    "        self._val_size=val_size\n",
    "        self._BERT_MODEL_PATH = os.path.join(self._DATA_DIR, bert_tf_model)\n",
    "        self._tokenizer = BertTokenizer.from_pretrained(self._BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "\n",
    "    def tf_to_pytorch_model(self,pytorch_model_bin='pytorch_model.bin'):\n",
    "        BERT_MODEL_PATH = os.path.join(self._DATA_DIR, self._BERT_MODEL_PATH)\n",
    "        convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "                                    os.path.join(self._BERT_MODEL_PATH, 'bert_model.ckpt'),\n",
    "                                    os.path.join(self._BERT_MODEL_PATH, 'bert_config.json'), \n",
    "                                    os.path.join(self._WORK_DIR, pytorch_model_bin))\n",
    "\n",
    "        shutil.copyfile(os.path.join(BERT_MODEL_PATH, 'bert_config.json'), \\\n",
    "                        os.path.join(self._WORK_DIR, 'bert_config.json'))\n",
    "        # This is the Bert configuration file\n",
    "        bert_config = BertConfig(os.path.join(self._WORK_DIR, 'bert_config.json'))\n",
    "        return bert_config\n",
    "    \n",
    "#     def convert_lines(self,example):\n",
    "#         tokenizer = self._tokenizer\n",
    "#         max_seq_length = self._MAX_SEQUENCE_LENGTH-2\n",
    "#         all_tokens = []\n",
    "#         longer = 0\n",
    "#         for text in tqdm_notebook(example):\n",
    "#             tokens_a = tokenizer.tokenize(text)\n",
    "#             if len(tokens_a)>max_seq_length:\n",
    "#                 tokens_a = tokens_a[:max_seq_length]\n",
    "#                 longer += 1\n",
    "#             one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "#             all_tokens.append(one_token)\n",
    "#         print(longer)\n",
    "#         return np.array(all_tokens)\n",
    "    \n",
    "    def predict_from_pretrained_model(self):\n",
    "        bert = BertModel.from_pretrained(self._WORK_DIR).cuda()\n",
    "        bert_output = bert(torch.tensor([input_ids]).cuda())\n",
    "        return bert_output\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        tokens = self._tokenizer.tokenize(text)\n",
    "        tokens_bert = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens_bert)\n",
    "        return input_ids,tokens,tokens_bert\n",
    "    \n",
    "\n",
    "    def initialize_model_for_training(self,num_labels,EPOCHS=1,model_seed=21000,lr=2e-5,batch_size=32,\n",
    "                                      accumulation_steps=2):\n",
    "        # Setup model parameters\n",
    "        np.random.seed(model_seed)\n",
    "        torch.manual_seed(model_seed)\n",
    "        torch.cuda.manual_seed(model_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(self._WORK_DIR,cache_dir=None,num_labels=num_labels)\n",
    "        model.zero_grad()\n",
    "        model = model.to(device)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        train = train_dataset\n",
    "        num_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n",
    "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                             lr=lr,\n",
    "                             warmup=0.05,\n",
    "                             t_total=num_train_optimization_steps)\n",
    "\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "        model=model.train()\n",
    "        return model,optimizer,EPOCHS\n",
    "\n",
    "    def run_training(self,model,train,optimizer,EPOCHS=1,batch_size=32,accumulation_steps=2):\n",
    "        tq = tqdm_notebook(range(EPOCHS))\n",
    "        for epoch in tq:\n",
    "            train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "            avg_loss = 0.\n",
    "            avg_accuracy = 0.\n",
    "            lossf=None\n",
    "            tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "            optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n",
    "            for i,(x_batch, y_batch) in tk0:\n",
    "                y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
    "                loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                    optimizer.step()                            # Now we can do an optimizer step\n",
    "                    optimizer.zero_grad()\n",
    "                if lossf:\n",
    "                    lossf = 0.98*lossf+0.02*loss.item()\n",
    "                else:\n",
    "                    lossf = loss.item()\n",
    "                tk0.set_postfix(loss = lossf)\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "                avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
    "            tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "            return model\n",
    "       \n",
    "    def predict(self,model,X_val,batch_size=32):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=False\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((len(X_val)))\n",
    "        valid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        tk0 = tqdm_notebook(valid_loader)\n",
    "        for i,(x_batch,)  in enumerate(tk0):\n",
    "            pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
    "            valid_preds[i*batch_size:(i+1)*batch_size]=pred[:,0].detach().cpu().squeeze().numpy()\n",
    "        return valid_preds\n",
    "    \n",
    "    def compute_auc_score(self,y, predictions):\n",
    "        return roc_auc_score(y, predictions)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be the files you downloaded earlier when you ran `download.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['download.sh',\n",
       " 'cased_L-12_H-768_A-12',\n",
       " 'test.csv',\n",
       " 'train.csv',\n",
       " 'uncased_L-12_H-768_A-12']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_size = 10000\n",
    "# valid_size = 5000\n",
    "\n",
    "train_size = 1000000\n",
    "valid_size = 500000\n",
    "\n",
    "bert_obj = bert_training(train_size=train_size,val_size=val_size) # create an instance of the setup class\n",
    "DATA_DIR = bert_obj._DATA_DIR\n",
    "WORK_DIR = bert_obj._WORK_DIR\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall install pytorch BERT implementation.   \n",
    "If you would like to experiment with or view any code (purely optional, and not graded :) ), you can copy the files from the repo https://github.com/huggingface/pytorch-pretrained-BERT  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from pytorch_pretrained_bert import BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now load the model. When you run this, comment out the `capture` command to understand the archecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "bert_config1 = bert_obj.tf_to_pytorch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Translate model from tensorflow to pytorch\n",
    "# BERT_MODEL_PATH = os.path.join(model1._DATA_DIR, 'uncased_L-12_H-768_A-12')\n",
    "# convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "#                             os.path.join(BERT_MODEL_PATH, 'bert_model.ckpt'),\n",
    "#                             os.path.join(BERT_MODEL_PATH, 'bert_config.json'), \n",
    "#                             os.path.join(model1._WORK_DIR, 'pytorch_model.bin'))\n",
    "\n",
    "# shutil.copyfile(os.path.join(BERT_MODEL_PATH, 'bert_config.json'), \\\n",
    "#                 os.path.join(model1._WORK_DIR, 'bert_config.json'))\n",
    "# # This is the Bert configuration file\n",
    "# bert_config2 = BertConfig(os.path.join(model1._WORK_DIR, 'bert_config.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert needs a special formatting of sentences, so we have a sentence start and end token, as well as separators.   \n",
    "Thanks to this [script](https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming) for a fast convertor of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm_notebook(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the BERT tokenizer and convert the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1500000 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ed7d1614344bbdbfb2d9a9fb318426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1500000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "33724\n",
      "CPU times: user 27min 52s, sys: 8.28 s, total: 28min\n",
      "Wall time: 27min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SEED = bert_obj._SEED\n",
    "train_all = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\")).sample(train_size+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(train_all))\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "train_all['comment_text'] = train_all['comment_text'].astype(str) \n",
    "\n",
    "sequences =bert_obj.convert_lines(train_all[\"comment_text\"].fillna(\"DUMMY_VALUE\"))\n",
    "train_all=train_all.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "# train_all = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\")).sample(train_size+valid_size,random_state=SEED)\n",
    "# print('loaded %d records' % len(train_all))\n",
    "\n",
    "# # Make sure all comment_text values are strings\n",
    "# train_all['comment_text'] = train_all['comment_text'].astype(str) \n",
    "\n",
    "# sequences = convert_lines(train_all[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "# train_all=train_all.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how the tokenising works in BERT, see below how it recongizes misspellings - words the model never saw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458232</th>\n",
       "      <td>It's difficult for many old people to keep up ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272766</th>\n",
       "      <td>She recognized that her tiny-handed husband is...</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339129</th>\n",
       "      <td>HPHY76,\\nGood for you for thinking out loud, w...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773565</th>\n",
       "      <td>And I bet that in the day you expected your Je...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476233</th>\n",
       "      <td>Kennedy will add a much needed and scientifica...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text    target\n",
       "458232  It's difficult for many old people to keep up ...  0.000000\n",
       "272766  She recognized that her tiny-handed husband is...  0.166667\n",
       "339129  HPHY76,\\nGood for you for thinking out loud, w...  0.000000\n",
       "773565  And I bet that in the day you expected your Je...  0.500000\n",
       "476233  Kennedy will add a much needed and scientifica...  0.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all[[\"comment_text\", 'target']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tokenize some text (I intentionally mispelled some words to check berts subword information handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi , i am learning new things in w ##25 ##1 about deep learning the cloud and te ##h edge .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hi, I am learning new things in w251 about deep learning the cloud and teh edge.'\n",
    "input_ids,tokens,tokens_bert = bert_obj.tokenize(text)\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added start and end token and convert to ids. This is how it is fed into BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi , i am learning new things in w ##25 ##1 about deep learning the cloud and te ##h edge . [SEP]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'101 7632 1010 1045 2572 4083 2047 2477 1999 1059 17788 2487 2055 2784 4083 1996 6112 1998 8915 2232 3341 1012 102'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens1 = [\"[CLS]\"] + tokens1 + [\"[SEP]\"]\n",
    "' '.join(tokens_bert)\n",
    "' '.join(map(str, input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When BERT converts this sentence to a torch tensor below is shape of the stored tensors.  \n",
    "We have 12 input tensors, while the sentence tokens has length 23; where are can you see the 23 tokens in the tensors ?... **Feel free to post in slack or discuss in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokens ['hi', ',', 'i', 'am', 'learning', 'new', 'things', 'in', 'w', '##25', '##1', 'about', 'deep', 'learning', 'the', 'cloud', 'and', 'te', '##h', 'edge', '.']\n",
      "Number of tokens 21\n",
      "Tensor shapes : [(1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768), (1, 23, 768)]\n",
      "Number of torch tensors : 12\n"
     ]
    }
   ],
   "source": [
    "# put input on gpu and make prediction\n",
    "bert_output = bert_obj.predict_from_pretrained_model()\n",
    "print('Sentence tokens {}'.format(tokens))\n",
    "print('Number of tokens {}'.format(len(tokens)))\n",
    "print('Tensor shapes : {}'.format([b.cpu().detach().numpy().shape for b in bert_output[0]]))\n",
    "print('Number of torch tensors : {}'.format(len(bert_output[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a binary problem, we change our target to [0,1], instead of float.   \n",
    "We also split the dataset into a training and validation set, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all['target']=(train_all['target']>=0.5).astype(float)\n",
    "# Training data - sentences\n",
    "X = sequences[:train_size] \n",
    "# Target - the toxicity. \n",
    "y = train_all[['target']].values[:train_size]\n",
    "X_val = sequences[train_size:]                \n",
    "y_val = train_all[['target']].values[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=train_all.tail(valid_size).copy()\n",
    "train_df=train_all.head(train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From here on in we would like you to run BERT.**   \n",
    "**Please do rely on the script available -  [Kaggle kernel](https://www.kaggle.com/yuval6967/toxic-bert-plain-vanila) from [yuval r](https://www.kaggle.com/yuval6967) - for at least the first few steps up to training and prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1)**   \n",
    "**Load the training set to a training dataset. For this you need to load the X sequences and y objects to torch tensors**   \n",
    "**You can use `torch.utils.data.TensorDataset` to input these into a train_dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data creations\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)**  \n",
    "**Set your learning rate and batch size; and optionally random seeds if you want reproducable results**   \n",
    "**Load your pretrained BERT using `BertForSequenceClassification`**   \n",
    "**Initialise the gradients and place the model on cuda, set up your optimiser and decay parameters**\n",
    "**Initialise the model with `apex` (we imprted this as `amp`) for mixed precision training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for training \n",
    "model1,optimizer1,epochs = bert_obj.initialize_model_for_training(y.shape[1],EPOCHS=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)**  \n",
    "**Start training your model by iterating through batches in a single epoch of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d48ab00648643048e22a7311bbdc839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 23min 39s, sys: 28min 40s, total: 1h 52min 20s\n",
      "Wall time: 1h 52min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "model1=bert_obj.run_training(model1,train_dataset,optimizer1,EPOCHS=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)**  \n",
    "**Store your trained model to disk, you will need it if you choose section 8C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_file = WORK_DIR+\"/bert_pytorch_v100a_train\"+str(train_size)+\".bin\"\n",
    "torch.save(model1.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)**   \n",
    "**Now make a prediction for your validation set.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following 2 lines are not needed but show how to download the model for prediction\n",
    "output_model_file = WORK_DIR+\"/bert_pytorch_v100a_train\"+str(train_size)+\".bin\"\n",
    "model = BertForSequenceClassification(bert_config1,num_labels=y.shape[1])\n",
    "model.load_state_dict(torch.load(output_model_file))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47166d621f20437d891ecf74cbcf662c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15625), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 20s, sys: 3min 26s, total: 15min 46s\n",
      "Wall time: 15min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions=bert_obj.predict(model,X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)**  \n",
    "**In the yuval's kernel he get a metric based on the metric for the jigsaw competition - it is quite complicated. Instead, we would like you to measure the `AUC`, similar to how you did in homework 04. You can compare the results to HW04**  \n",
    "*A tip, if your score is lower than homework 04 something is wrong....*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score =  0.97004\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.sigmoid(torch.tensor(predictions)).numpy()\n",
    "auc_score = bert_obj.compute_auc_score(y_val,predictions)\n",
    "print(\"AUC score = \" , round(auc_score,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)**  \n",
    "**Can you show/print the validation sentences predicted with the highest and lowest toxicity ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  1\n",
      "True Target Value: 1.0 Predicted Target Value: 0.9995\n",
      "\n",
      "\"An idea that seemed plausible and gathered a lot of excitement when it was proposed decades ago lost its traction as the 2008 economic crash took its toll on the RTD system.\"\n",
      "\n",
      "This is absolute hogwash. The northwest rail line was dead in the water before the 2008 crash ever happened. Most of the same factors that make the line extremely unlikely to be built in most of our lifetimes existed at the time Fastracks was being proposed. So why was it part of the plan? Because Crooked Cal Marsella knew he needed a big turnout from Boulder and the northwest metro area in order to win at the ballot box.\n",
      "\n",
      "My frustration is not that the northwest line won't be built, it's that RTD didn't follow through on its Bus Rapid Transit promises (no, the Flatiron Flyer isn't BRT - it's the BV, BF and BX with a new paint job) and continues to make bus service cuts in this area. And if we contact our RTD rep about it, she's too dimwitted to respond with anything other than a staff-written form letter.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  2\n",
      "True Target Value: 1.0 Predicted Target Value: 0.9995\n",
      "\n",
      "Buhbye loser.   Rent that forehead out as ad space and earn a real living, you tool.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  3\n",
      "True Target Value: 0.0 Predicted Target Value: 0.9995\n",
      "\n",
      "Is there any evidence that indigenous doctors or lawyers would be more likely  to stay on a remote reserve?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  4\n",
      "True Target Value: 0.0 Predicted Target Value: 0.9995\n",
      "\n",
      "I think we may be missing something with this program.  We are going to provide \"free\" breakfasts to all children but as many as 50% of the families need no public assistance.  We are going to do this in the hope that those families who don't need the assistance choose to have their kids participate in the program.  We can pretty much count on many families not participating, so their kids will be \"different\" or sitting there with other kids maybe/maybe not eating and being kids.  Quite honestly, this reads more like a federal dollars grab to save a program that the current budget can't pay for and/or an organization plan for teachers and administrators to herd all kids into one location as opposed to many.  I sure hope the food is nutritious/edible and isn't wasted because this is going to happen whether sound social/fiscal policy or not.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  5\n",
      "True Target Value: 0.0 Predicted Target Value: 0.9995\n",
      "\n",
      "Great column, Charles. Your parents are a tough act to follow, but I think you're doing a pretty good job. Keep trying to inspire us to be better. We'll do the same for you.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idx_most_toxic=predictions.argsort()[-5:][::-1]\n",
    "for index,row in enumerate(idx_most_toxic):\n",
    "    print(\"Sentence: \",index+1)\n",
    "    print(\"True Target Value:\",train_all.iloc[row].target,\"Predicted Target Value:\",round(predictions[row],4))\n",
    "    print()\n",
    "    print(train_all.iloc[row].comment_text)\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  1\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "Mr. Speaker,\n",
      "\n",
      "Please inform our Russian friends that no one in Canada votes for the Prime Minister except the people in the Prime Minister's electoral riding.\n",
      "\n",
      "Prime Minister Trudeau received  almost 52% of the popular vote in his riding, a majority of votes cast.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  2\n",
      "True Target Value: 1.0 Predicted Target Value: 0.0001\n",
      "\n",
      "In a business world (Donaldland) bottom line is $$$$. Lots of $$$$$. So why not deal with Russia businesslike? Not politics but busine$$. That's what's happening. Hey!! Mr.Day One Trump, show us the $$$. Words are cheap. Promises are cheap. Bankrupcies is just a tool we in business use.  Flying to Florida this weekend?? Lol, you know Mar A lago is just a secret palace where deals get made. Lol. You voted for this clown.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  3\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "Belittling her because she has an accent, or because. She wore hit pink, or because her shoes did nit comply with your personal fashion list is such a small minded approach.  She could have been. Shaksperean spokesperon, wore black, and flats and many of you still would reject her.\n",
      "So glad I don' know any of you.  What bores you all are.\n",
      "Contribute valuable/worthy comments or just zip it.  Third graders make a better argument.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  4\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "Well, according to a May 5, 2015 KVAL report, EPD had 102 untested rape kits in their possession at that time.   And those 102 kits  were only the ones \"collected in the preceding three years\" by EPD.  They weren't \"at the state police lab languishing\".  They were at EPD.\n",
      "\n",
      " Your conclusion?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sentence:  5\n",
      "True Target Value: 0.0 Predicted Target Value: 0.0001\n",
      "\n",
      "Eyewitness accounts:\n",
      "http://www.thedenverchannel.com/news/local-news/dps-responds-to-alleged-confederate-flag-and-racial-slurs-at-football-game\n",
      "Including a quote about asking DPS Weld to remove the flag, racial slurs from the field heard in the stands.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idx_least_toxic=predictions.argsort()[:5]\n",
    "for index,row in enumerate(idx_least_toxic):\n",
    "    print(\"Sentence: \",index+1)\n",
    "    print(\"True Target Value:\",train_all.iloc[row].target,\"Predicted Target Value:\",round(predictions[row],4))\n",
    "    print()\n",
    "    print(train_all.iloc[row].comment_text)\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8)**  \n",
    "**Pick only one of the below items and complete it. The last two will take a good amount of time (and partial success on them is fine), so proceed with caution on your choice of items :)** \n",
    "  \n",
    "  \n",
    "**A. Can you train on two epochs ?**\n",
    "\n",
    "**B. Can you change the learning rate and improve validation score ?**\n",
    "   \n",
    "**C. Make a prediction on the test data set with your downloaded model and submit to Kaggle to see where you score on public LB - check out [Abhishek's](https://www.kaggle.com/abhishek) script - https://www.kaggle.com/abhishek/pytorch-bert-inference . Note, you will need to fork Abhisheks kernel, swap out the weights to your downloaded weights and commit the kernel. When finalised and you get the output, there is a button to submit to the competition**  \n",
    "  \n",
    "**D. Get BERT running on the tx2 for a sample of the data.** \n",
    "  \n",
    "**E. Finally, and very challenging -- the `BertAdam` optimiser proved to be suboptimal for this task. There is a better optimiser for this dataset in this script [here](https://www.kaggle.com/cristinasierra/pretext-lstm-tuning-v3). Check out the `custom_loss` function. Can you implement it ? It means getting under the hood of the `BertForSequenceClassification` at the source repo and implementing a modified version locally .  `https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Training on 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097f3e059a014b07a5add4c41368e9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03259426a23f46a5a5c090ac17708169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15625), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score =  0.96963\n",
      "CPU times: user 1h 40min 53s, sys: 30min 9s, total: 2h 11min 3s\n",
      "Wall time: 2h 10min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize \n",
    "model2,optimizer2,epochs = bert_obj.initialize_model_for_training(y.shape[1],EPOCHS=2)\n",
    "\n",
    "# Train and save the model\n",
    "model2=bert_obj.run_training(model2,train_dataset,optimizer2,EPOCHS=epochs)\n",
    "output_model_file = WORK_DIR+\"/bert_pytorch_v100a_train\"+str(train_size)+\"_epochs_\"+str(epochs)+\".bin\"\n",
    "torch.save(model.state_dict(), output_model_file)\n",
    "\n",
    "# Make the predictions\n",
    "predictions = bert_obj.predict(model2,X_val)\n",
    "predictions = torch.sigmoid(torch.tensor(predictions)).numpy() # add a final sigmoid layer\n",
    "auc_score = bert_obj.compute_auc_score(y_val,predictions)\n",
    "print(\"AUC score = \" , round(auc_score,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Change the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5841a426be20486989581fa7002d4d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67bf5c0a2f14dc89b2ad4729c27c55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15625), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score =  0.96916\n",
      "CPU times: user 1h 44min 38s, sys: 28min 15s, total: 2h 12min 53s\n",
      "Wall time: 2h 12min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set learning rate \n",
    "lr = 1e-5\n",
    "\n",
    "# Initialize \n",
    "model3,optimizer3,epochs = bert_obj.initialize_model_for_training(y.shape[1],EPOCHS=1,lr=lr)\n",
    "\n",
    "# Train and save the model\n",
    "model3=bert_obj.run_training(model3,train_dataset,optimizer3,EPOCHS=epochs)\n",
    "output_model_file = WORK_DIR+\"/bert_pytorch_v100a_train\"+str(train_size)+\"_lr_\"+str(lr)+\"_epochs_\"+str(epochs)+\".bin\"\n",
    "torch.save(model.state_dict(), output_model_file)\n",
    "\n",
    "# Make the predictions\n",
    "predictions = bert_obj.predict(model3,X_val)\n",
    "predictions = torch.sigmoid(torch.tensor(predictions)).numpy() # add a final sigmoid layer\n",
    "auc_score = bert_obj.compute_auc_score(y_val,predictions)\n",
    "print(\"AUC score = \" , round(auc_score,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
